{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npIrOYmGWU7b"
      },
      "source": [
        "# Model Quantization\n",
        "\n",
        "\n",
        "Here I implemented post-training quantization approaches for Large Language Models, ranging from naive ones to State of The Art techniques. The main achievement is implementation of [GPTQ](https://arxiv.org/abs/2210.17323).\n",
        "\n",
        "**Requirements:**\n",
        "Colab with T4 gpu with at least 15Gb of *VRAM* and 12Gb of *RAM*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld5GgOmT2WE1"
      },
      "source": [
        "# Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2v6kSVkV2cSS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install transformers==4.35.0\n",
        "%pip install sentencepiece==0.1.99\n",
        "%pip install datasets==2.14.6\n",
        "%pip install accelerate==0.24.1\n",
        "%pip install Ninja==1.11.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtpmhiksi62J"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBpuR3vpi62J",
        "outputId": "0053dfca-bc10-4da9-9c3f-c9f73e634c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
            "env: CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "%env CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import Mapping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM\n",
        "from transformers.models.llama.configuration_llama import LlamaConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQkjY8Jp1OpH"
      },
      "source": [
        "# Quantizing Matrices Row-Wise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzEoWTNMi62K"
      },
      "source": [
        "### Basic Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFG1FDsVi62L"
      },
      "source": [
        "**Mapping the values to the allowed range**\n",
        "\n",
        "Quantization is the process of mapping input values from a large set to output values in a smaller set. For instance, if we consider 4-bit\n",
        "quantization, our values are represented by $4$ bits, meaning we can represent values between 0 and $2^4-1=15$.\n",
        "\n",
        " * To produce the quantized representation, we need to be able to map the matrix values to and from this range.\n",
        " * For reasons that become important later, we will perform this mapping independently for each matrix row.\n",
        " * We will parametrize the mapping like this: $out = \\frac{in}{scale} + zero$, where $scale$ and $zero$ are row-wise constants.\n",
        " * For a matrix of size `(m, k)` ($m$ rows, $k$ columns) we will aggregate those parameters into two vectors `scale` and `zero` of size `(m, 1)`.\n",
        "\n",
        "Here is the mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1xDv2d12hw9"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produces tensors scale and zero of shape (m, 1)\n",
        "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
        "    min_val = torch.min(x, dim=-1).values\n",
        "    max_val = torch.max(x, dim=-1).values\n",
        "    scale = (max_val - min_val) / max_abs\n",
        "    scale[scale == 0] = 1\n",
        "    zero = - min_val / scale\n",
        "    return scale.unsqueeze(-1), zero.unsqueeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ8TZiGyi62M",
        "outputId": "1993dcf5-65b4-441c-a6e6-62ae05a20fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "assert scale.shape == (512, 1), \"scale is wrong shape\"\n",
        "assert zero.shape == (512, 1), \"zero is wrong shape\"\n",
        "assert torch.all(scale * 15 <= 1023.1), \"Scale can't be that large. The resulting interval is too wide\"\n",
        "assert torch.all(scale * 15 >= 1022.9), \"Scale shouldn't be that small. The resulting interval is too narrow\"\n",
        "assert torch.all(-0.001 <  x / scale + zero) and torch.all(x / scale + zero < 15 + 0.001)\n",
        "\n",
        "x = torch.zeros(128, 128)\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "assert torch.all(scale == 1) and torch.all(scale * 15 >= 0.99), \"If all the values in a row are identical, let us set scale to 1\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1DfQaSi62N"
      },
      "source": [
        "**Quantization**\n",
        "\n",
        "Having mapped the values into the allowed range, we can simply round them to obtain the quantized matrix.\n",
        "Important:\n",
        " * I `torch.clamp(...)` the quantized values to ensure that they are in the allowed range.\n",
        " * Some functions return the quantized matrix, as well as the quantization constants, because we'll need them to dequantize the matrix.\n",
        " * Note that we cast the quantized tensor to `uint8`, but the values themselves must be in the possibly narrower range, as determined by the number of bits. Obviously, we require the latter to be less or equal to 8.\n",
        "\n",
        "Functions for row-wise quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZR0OZ0ai62N"
      },
      "outputs": [],
      "source": [
        "def quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n",
        "    \"\"\"Quantizes a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "        bits (int): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        Tensor: quantized tensor in uint8\n",
        "    \"\"\"\n",
        "    max_abs = 2**bits - 1\n",
        "    quantized_x = torch.round(x / scale + zero)\n",
        "    quantized_x = torch.clamp(quantized_x, 0, max_abs)\n",
        "    return quantized_x.to(torch.uint8)\n",
        "\n",
        "\n",
        "def dequantize(quantized_x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n",
        "    \"\"\"Dequantizes a tensor\n",
        "    Args:\n",
        "        quantized_x (Tensor): quantized tensor in uint8\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "\n",
        "    Returns:\n",
        "        Tensor: dequantized tensor\n",
        "    \"\"\"\n",
        "    return quantized_x * scale - zero * scale\n",
        "\n",
        "\n",
        "def measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Determines the values interval mapping parameters and quantizes a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        bits (float): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n",
        "    \"\"\"\n",
        "    max_abs = 2**bits - 1\n",
        "    scale, zero = get_scale_and_zero(x, max_abs)\n",
        "    quantized_x = quantize(x, scale, zero, bits)\n",
        "    return quantized_x, scale, zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM8eIsR8i62O",
        "outputId": "ae82b48a-ee50-4b88-81a4-9aa19c9d5ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "quantized_x, scale, zero = measure_and_quantize(x, 4)\n",
        "\n",
        "assert quantized_x.shape == x.shape, \"Shape of quantized_x is incorrect\"\n",
        "assert scale.shape == (512, 1), \"Shape of scale is incorrect\"\n",
        "assert zero.shape == (512, 1), \"Shape of zero is incorrect\"\n",
        "assert torch.all(quantized_x >= 0) and torch.all(quantized_x <= 15) and torch.any(quantized_x == 15), \"wrong quantized_x values range\"\n",
        "assert torch.allclose(x, dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBDvbhJSi62O"
      },
      "source": [
        "**Using the quantized matrix**\n",
        "\n",
        "To actually use the matrix, we'll have to map it's values back into their original form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NMzGxzt-i62P"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, scale, zero, bias):\n",
        "        super().__init__()\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "        self.zero = nn.Parameter(zero, requires_grad=False)\n",
        "        self.bias = nn.Parameter(bias.data.clone().detach()) if bias is not None else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, dequantize(self.quantized_weight, self.scale, self.zero), self.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncM_JzOii62P"
      },
      "source": [
        "This class will be used as a replacement for `nn.Linear`. It holds the quantized weight and only dequantizes it during it's forward passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPaMiUC91VYH"
      },
      "source": [
        "# LLM Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HRqro7X1eoT"
      },
      "source": [
        "### Preparations\n",
        "\n",
        "Downloading and preparing the model and the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipghRjSA1jVN"
      },
      "source": [
        "**Downloading the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JIUbMToO3C9B"
      },
      "outputs": [],
      "source": [
        "!mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d10e4667e06d42a5af907423fce3783b",
            "3a873fa120734f838cb7d407d54db37c",
            "5fc31d5379f647d58520cac19fbc3eaa",
            "2a8edf17aa4143598814e650280b2085",
            "094fea41a7604d9db5b6e74e4485775d",
            "41e64c8226a448718391411455463937",
            "d1dc17b8cae64caebc7aed8a8fc8fbca",
            "20011a533493400299e1b5ee2bf9d905",
            "10b3149126d24c7e978a6cb667bf9ce7",
            "759bf11897da4ba6846bf6d16ecfb558",
            "00688616bbc74e46a6925fe589efa502",
            "646d70693e0e4bae886baff55cfa87e5",
            "bdd01cdf82fe457dbd54b9a89bc39d93",
            "5076e79b211a4820b055f7afa54e6480",
            "96079658b8a74b38b08c4c0ea289066e",
            "b6d38390ad7c4373829d38121aae2f53",
            "c211b10d0f5545d9bf43d304f2738f54",
            "02caee64c6944393b2d67bd42b01944d",
            "b900ac520daf47e785bd2657df18fd40",
            "d62bdf18725846ad929ba437bd1d8202",
            "4c0a9e00ef5a433ea6bb9c5d61f6a038",
            "43185a93875e40a3a9d6a3e9069a0b7a",
            "45421d55c3254aa2999ba25519320004",
            "9a69a9feac0f4c14930afbae233818e9",
            "0db4052da0d347f1a2e488c5f1019895",
            "8c5f2d577f5b4cd2b6ad67e3d39912e5",
            "976e4a205dc64631aaa00d900ee103b7",
            "dca56bd203bd4023ba2aa9f52a590d98",
            "a6c32f8a06a445e4b57f7569c5fb48cf",
            "14668052f46c4805ba617009863f9077",
            "90decf0705624e1bb1492ef581b40c8c",
            "22e46d7ba70c4e8f8c306861ac8228b8",
            "f78364c6e7354b71b541a38c111354a9",
            "585b344bb77d44e594ba1c91b545aa60",
            "6564f5f43f0a4161bf6a50b2dcdfed2f",
            "14c2f68789274be095867154b88cc31f",
            "784f610cdca24ed882e9f46fd93eb247",
            "14690f6112874ab99a1591bc6aaf1507",
            "4206b9ddcae142f89297193f472e18ce",
            "52b9aa244e6047b7b97f4e351a62ecba",
            "baa2ef3fd9b84b55b38da4fae97ecd9e",
            "9f881be13664481b997ee966392c0bf3",
            "d2c33d9c202c42ad8f3c84d4f7a393a7",
            "1939f06a174c4ba1afac2d70c71d9976",
            "696af3809c2644778ca0634a8276cb23",
            "9c0c218c37814ea8b99317aa2c8dc177",
            "7a24015374804677974ae7ed3f82bf04",
            "f80dfe8ccdc449189e7f3da5078dacfa",
            "526fc46402324d7a8e14c14c5080dae6",
            "a4c779903d8b4facbcffb927834c1ca2",
            "d061c6882a7a43ada89d074ed0621c34",
            "3fb8b1ec38fb4be28a011d3521e4dffb",
            "2bc9dd3bcc4b4d1e96f2345886e7f749",
            "c6555e2515f84d9fb3b8c6ec258dde0a",
            "1aeca7d510c54161894783ac924e301b",
            "d6b9b821b0144aef9368a4618b495cbe",
            "836a7f9d40bf4513b6b66841eeafc706",
            "6f4d5a43ef6a4356afe41fee44bc805c",
            "78d8b719dc1a4616a254463fddaf63bb",
            "7d4c3b65e997499cb64a40074fc080a9",
            "2cbfe8abf0a649bea394b9449cf6648e",
            "a25e8c59a030417e8fdbd0c9921a268c",
            "ced8a80341544dabb79341c39b962484",
            "3da5027283254a29a8afada15f304a98",
            "90ec60146d7b4e55a48ad475297c0bfd",
            "438ecbface24488ea72e83819eb0034b",
            "7d24f6615b874052a441d4de4738e9a5",
            "0964f3d2cc144a1fa70626d415417734",
            "8f47327419c844f5bdbcd7a3e8780e9b",
            "a423fefcb2854cda84909f133c0c1406",
            "ccb2ad143faf4cf6b36df195184ff67b",
            "0c994863155744009edd5e3b79e58847",
            "925fcd8395bb48fb96f95a91859240e2",
            "0af570e9abf844ddad00ba7fd0859516",
            "685aa216fcbf4b83a7a1ac587e1fb70f",
            "87a3fb717b744c2ca44cf4f2a8444624",
            "34007bf580034bfba6a354b434db6632",
            "f7a63c92916d4788aa8a93a6ec1678bf",
            "9627b9d611fa47e0a6ac1b510e13d952",
            "1e2684c3564f4822af8ff24d2cc3d69c",
            "dfd04de377684be1a17c206ade4c6e22",
            "afb26eccda02439cb83f1f366452439c",
            "fccfa731e9524a76a079705934349627",
            "b9b951318ad94d3f839d94e043b7ea05",
            "bbc68d0318514b8bbd8a74e5e851f02e",
            "232962892a2b43558586c5a3aa5eabf6",
            "e111febf4f6944ba9238d36d9868f65b",
            "9f8dc5f04439480b93f36270c27d645d",
            "ac6fccad33c1432191f0820b8bf4c919",
            "e390a88433db4cd3a507a13ebd81ec1b",
            "4312782699264333b5160dd6522db25b",
            "edf1a6a282af4815aeaa827536918b7c",
            "9fa4cd4eab224263933edb98bffb13be",
            "fcfd53f192744e7ca6793fa17444c5de",
            "9c2b71a7f1af43f2844784634dc6229f",
            "2369b2d12e754caf80229330ce22b65e",
            "489e7362811345b18d25cb2eeff9c69b",
            "76e39880dd4e4e9ab192724439364267",
            "0de7785980464099a3b24379b711d4b7",
            "571bd1f2240d4bc4acb91c0929e5fdd5",
            "8c18ccec57da4ef8be89aae95c276da6",
            "b26429b847ab43e793e8ac863d8e6b3b",
            "39c52830b4e94a30b5e1f85e4eacc4f6",
            "b0efa23d877442038efd2e999d6b37c3",
            "0b7abdfbbb42497b8944e3713b1b78ad",
            "bffab43cc99d4e3fb63133c597b4f7f7",
            "c5684a0e8f4f4b83bca2dd0696bb7035",
            "964c14d07ff04a1d82b3a639acedda37",
            "11ba1266ab4c4850bc798a5661fc2181",
            "bfe959c963174486aec4edbe96ab862f",
            "0d29fa3d3a694a809964403e662d5ba8",
            "ecdfd56cf7db482aa893fd384e7ec730",
            "0997b2d1bda4488bbd9f31a7f9539aa7",
            "7c5e266e79414f9eb90ab67d9b4e2158",
            "be1fbd2a8fbb42279ab4922a203d557a",
            "8b6fe7adcaf042289324f0f85f113308",
            "1abd8b59b0cf453cac2b7bbd9b60a3ea",
            "7e12eab3de8146b6a28bc77f218816df",
            "5b1c6f3cd8ef4a8f83ea28f7674fff90",
            "79e3e3df41554bcf904f6051c9fcfca8",
            "dfa718a4a19240ea80750979911076c8",
            "e0134142b6174e17b200073cbe7d279b",
            "8d02e6dbba7e47cc8a74cad912a1ce50",
            "9d900391863842f086e346f10c4cada2",
            "0b3c9534f91d45539860e8fef7370997",
            "20d333de06e248f0af6ecf7b8e8449ff",
            "d699ce636c074a88ba800566c5635ea7",
            "7adffcca0d634b118cd8cc48547464c7",
            "9c06a85ef78d4d819b8e0a32d6e6bd14",
            "f8f08a580b81437882ac5e6718c082d2",
            "da928142019e436685205d7045350758",
            "39eff30f11184be4999b8f7b7b18f3ea",
            "abbddae7703f492dad1cb8690060a6f3",
            "bb5c3cb2945b49f78cd1271b08fed096",
            "f531fe369c7d49caa1c0cf9f71ce47a2",
            "ae23eef345ac4d66bad59530ee329bb7",
            "52b4934096b74872888f30a1277732be",
            "655b731f1cb64b0fa67820f9ade410d5",
            "95dfbb36846b4d448e842e7af8961584",
            "c602a29198ac429bbe2d5c1c124e45ed",
            "72159906b77a484c8e8528a533c0b988",
            "119374da773a400c9911d6b806c7a553",
            "e4b5ec2399fd4f7fad8d8257369c6f12",
            "5ecbb64715154c12830129a59db40cad",
            "6d9e4aba5ae344dda45d3458d0cdc9e6",
            "3b7f70fd54ca44ad933e3f9d7bfdfa0b",
            "b9b8413e1a264568996ad911047647d2",
            "967ebf695f7f455cbcc5ec3f05d46aab",
            "581acefa225a4a92a5df28ca558db958",
            "2850b5083abe4df19705892c00b4f847",
            "f7bf7923f3c34e488e766c399df86ffd",
            "6c51e59fa3fe4cf7b944f5ba706755b3",
            "1c98c68c8405427cbcbb4e7e3a1ad620",
            "a70d03f4669a41e290a7a750bffdc0dd",
            "92ba695ac3bd45599b38bc44eb0ee489",
            "cbd07cd458f34bc9ac00860178882186",
            "df3b4418b3ad4338afa9cc72b46b7e8b",
            "ccda1eca4c8348a0b45fcedd56d86dcd",
            "ce014566305a451b9eb03af16928e0b7",
            "6c0a048126aa4b2baaa3b04bed1fd9f5",
            "207e4d985c2b41f894931a6627be919c",
            "fdebfb01a28d401e82b965da8e62def0",
            "3ea306442eb14d719fcbf405c442c8cd",
            "45e74af7666348dea070c1ccfeb98d06",
            "e526e718bc1848b381ef0698bd082198",
            "c12d6a11a7ef438daf6daa1e4120ffe7",
            "d836c744a76f495297ffeb51dfb5912c",
            "52f5437b40c24c6bbf9e6f520342a093",
            "4b1a5772869b40ee8864e2711154ad88",
            "bab0dc635d944bef8eb25e8a9229077f",
            "2462447423eb41b7a7c350c8ca598920",
            "51aa4c0d2b44493289b70da379098de5",
            "7956f87afaa340caafb26557a57190c0",
            "610404fe78c24d54adeddf4d8c5ba491",
            "43de46835ead4039bae0bc82bccef4ee",
            "0c9db210c3834cfc9cb74ee0dea9947d",
            "6e44519cbd1f4c7db66cf578184aedea",
            "ce4ccb3121e842a69d58c886acb62c4d",
            "c6589065399644e290c76eac4c532b50",
            "c57cfec772204142ba70ce6ceb71f3bc",
            "c26bf438a1694e79bfe59df2f3383aec",
            "e82f9afcd2474e4a95329bd3f1b89753",
            "0091fc3f76c4428f855e7d21e9600610",
            "f515ecee3a8a41499f779813d833688a",
            "d5e1142fca57448bb1e41e4032158874",
            "ba53c7e238974610a539d0a64fc00b4f",
            "b07b2ca817394a1f997692afe0c1a6b4",
            "69e548e11c414e758a6ec676abd1f6ac",
            "1a2cebc8a2ae4febb3b9f94092a0286a",
            "540e6f76fc644903a237eb0250939999",
            "ba4c5f778d004e9dabaa53014145fa9e",
            "9b43670758e4419bb47b1d89703a06a3",
            "210faf61cb364c58bf5eee5a7837c6fa",
            "5abcd48ab7824f0d9966cefe4dc6e82a",
            "b87a1ad79d264e1fb784f8d918a6b841",
            "c804630c2f5d4fca869ac619e38e46a4",
            "a2f9edf6e67c42e28e69aa158d20968e",
            "bd25e11120864e9dab647b4d0132fb08",
            "d66300aa757a446a888f7cf9ee2d46c9",
            "b88f1294080d449686da997e25a6ddb8",
            "5b6fd7a8968e4d9aa02c601a19eb3524",
            "baad80249f59421a8e6bdbd937c0fdbf",
            "dbce9f5cfcd84939b992a07d7acab5ed",
            "4565541031fe4fdf8d424e008509bdc9",
            "d2a1d66fe26b4742a55016bda926f6f2",
            "b7f7a9668808428dba9b98e8e7e5b7c5",
            "3536890c0e06438fb6464bb5edc690c4",
            "4feea4445878498bbd2fc23e73ccb5ec",
            "abade44cdc434850bfa4b23d5375b866",
            "018aa17421ba4ca4bdcc32f98799cd73",
            "cbd982b4f95742c4816b1235e6c19325",
            "500ebafc86da4548bcbc46128cb200dc",
            "992f544c4cab40db9ebb7a0a57e3766a",
            "92e11367bb894382aa0c65ce75ad008f",
            "138e73358cc045e4b9a3263032e56c49",
            "15d51ec36dd54b909c48d6e94ca55b2c",
            "c348c6b5e42c4e54b7c9fb16b3d83717",
            "8096b7c390a84250b9fda96ba01d07a8",
            "39655dbb8c7a4457b743ecccb13e34ad",
            "1f646916c28c4d20bbacf4197611664f",
            "b97faf5b61cd488698d8843c84c82093",
            "631c01d912b840318f7606fe2be3ec05",
            "1de488dbf7f14865b116140106434845",
            "f7a3ff9ed4ff491c8aabc94649f7acf9",
            "f64731c4613c4df4919fd90da111d85d",
            "3bc4b7f68c8542fc99d29d27d481a172",
            "66784688c9c74ebc8ff2abbdde24b893",
            "e81fa2d901484f5889f1b5ae98b80f15",
            "21ffeb99675d4ad58057bc264210c6c4",
            "08a324a44f7e4831ab2eeed744c760d4",
            "95e2e6ca51184a668e4a34adbe178114",
            "10ca042ac9354718bbc149169e530a74",
            "0a921934c4ba4bab8d30dc6cc9c4cbc2",
            "81f876e24de54be787f52dabba8a062f",
            "2ec1d9659cad4a659c232e540435c040",
            "192e19500c2b4852aea2ccfd2a722f69",
            "e09dd5882173478db46cfecaac3dee2a",
            "64aa0d9b12a24619933431ab26963b04",
            "841b43b37f504a0fb7139a972608ed7a",
            "456718a83e80475f8d06c852dc8bd756",
            "17a50e92e600445ca9b0f05ceaf39cff",
            "61b96fc72b20476d868def0171e8ca85",
            "8d992050798f48778f783d901597cc1e",
            "c298f3b880944dd58bcf340ca0afae47",
            "bf52d3200eb64e52bf4fab8d281c3562",
            "48635b774c0f4c63b6550b93b0109807",
            "d2d3b6604f9d47ba84554bc56c1f5620",
            "9adf4fcb9a2e47789aa4109931672c27",
            "ef6ee4bb77db473f851955ee8346fa54",
            "0e2d09592b114ec283cff2120020f992",
            "4f08c436a630429ba6e32d4c5a9b477c",
            "2c855447a9cc4309a96a378a686054c5",
            "43a1de949d7c4fadb38afc7fe0f8b91b",
            "ed75f140049d4a398ed429a4eb4ec115",
            "6bd8ce76945d4989a827d445fbd19bbd",
            "0a70bb4ce0474b34b5a0af3b4702438c",
            "5bf38ec4d4d94eb9a4e71f1148304ab7",
            "59191faee0f1426491146a4e5f485196",
            "dd92fd76c4be4dc39652462d5b083b6b",
            "3e8b1d20812a41c6ba9b7c603d37d5ad",
            "308a4dc79a174eca97fe6ad458bb73a4",
            "c84390ba06c94f7293fa911c6e29461a",
            "34254f434d084de192ed3526e20527e0",
            "2f2476cbe0c443da8847e5d69928628f",
            "9386cc8f7f714fd8b8f07f6a89090fa7",
            "1aa0add15fc24c77afe3d787dcdd2809",
            "72bf0a9c0d124a04b437bdaea6907cd7",
            "e05fed89da4c47cfbeb284cd43700f0c",
            "3f0ca57f238b43a69d7c15554e41bf65",
            "6da3ffd99b9e4cbc92886577814fafad",
            "d93266f63c434aa2a2994e9314f8a26d",
            "ccf94c3bbd7948f6914ef2e0ff143728",
            "7bee4a40b43b40afb93f48edef88f8cb",
            "10ea720a438b49138af688be6d0074e4",
            "868c6aa9ffb546a88a4812378bc3f313",
            "4a123076360446138a1cb2fa2886bea8",
            "30bd6812bb9e456082285535b237f259",
            "dc54d7c35156488b84ae4317d6a956f1",
            "93a8f4ff6da34b789ada407b246dcf79",
            "9160b20160224703a6ec77b47cfae6f4",
            "63142877f4bf43258988678bfb227363",
            "179249ade48043588c17989ab7dda586",
            "bd38518db04d4ba78599c2c57212c200",
            "08bf64cb871d4063813391736178b6fb",
            "24be8c6443434d3ea32da529cabc3909",
            "b88d114a7b7b4ba98b8a5c491b5c7811",
            "dfc951366e184512a964bc615c228fdb",
            "7644b90dacf746ee986a3542838df0eb",
            "d0ead14d77bf492686c2105e822e8143",
            "0257445accd1457ca0c4b8364318bb3d",
            "c791d6bf37de4ab2a8bb7210800796fc",
            "17a15b5dda504bdbb4906f79c4f5b0bb",
            "266ae3cd8ea143c4bc591e1fe437853a",
            "513607123cb140b8a4c411bcc46e8b8b",
            "1a342cfc112642b296d0e6fd01d69ffe",
            "146bbfc6ed284f8dba9937e04a5952b7",
            "4295bf9af4e441618c9feb42be7afc80",
            "b1531d189c4e4c77b8ef43bc65170bc8",
            "4d82072cb58947f690d41897d0940e7b",
            "517094f9c1694c48a069c24406b41481",
            "44bcb2d690ff43e2b0e2c724bca8a43f",
            "8529801b4b264f999d33a433ee8c4b9d",
            "6ec422f1fe624a0daa90fa57ba2fb524",
            "d3e1d9fe819c4da2a0f2cab38cc91d15",
            "881b1bf3db2648c189bef187f726a538",
            "3f9da81bb9fa405892958e41aab6d1ac",
            "039bfd1e73c14abfbc5ededf5df9fa23",
            "efb6073271e148b4a0d0aa0cd24faba7",
            "de09f131efa24ae3a22009b79d04c99b",
            "6b9e3b21ea9f4c5599f07a32316ab587",
            "483f9ab3ade6433fb55dbf01d8aa7107",
            "7bf737b56b754e508b2df1173b01a5e5",
            "3cd4fd8de1c74939aad7f15bcfc347dc",
            "2f1ff84ab48442e5b65a5b383fcd636a",
            "3160f0a54ebc4711af2e99a734a38e25",
            "23ccad7cfee04df2ac1319da899465c5",
            "5a788abb6ef1481eaaad72d5a139d1fd",
            "5b3069f6eb7541f8ba01891142f8855f",
            "a354aac2d4bd445dbbf1499906fd8919",
            "79410ab5d8864684a2d8bd70d8849b9c",
            "6ea1076a45c84803839247c3b2cb5774",
            "a0293e8a512f4e34a94cd41b4030ab4b",
            "70857829d11a4bb2ac0757166b4e1da0",
            "3866bb4f93e2484ab31af85894469a0e",
            "442038889e234b1d9df07cb4a038aa94",
            "aec98bbe575a47d2ab9191f4eb1f05e4",
            "fd18da19b5744905934758c2e2807ec3",
            "fc010eecb8824397992fcf4f6ffc7cb6",
            "f37a55facf224da9a133a0e59fe06550",
            "a154ab35f0624b139fd56be866f0806b",
            "f070276d70da482c9325b0048c6d84e3",
            "4082e4dec6d04ee882d5e89986519611",
            "25098aca6d6f4accb79178079cf2b441",
            "cf0a44f5e9624754b4d5797307994018",
            "fb1d6a5723ec4088a96ad08ebdd7abb0",
            "304c270ce7d64b0584161bc3554c1d39",
            "3b729ed78dea4aea9e110b3129155f57",
            "880bfa0a50c84dcf8f1c8a461ef29d66",
            "0bb268c7cea14fb0b10700ac20dc37ca",
            "434baebfd20a40139801cc9ccb72d64c",
            "486eeae0c74e49b8bae4421d70272726",
            "b6e2ee767a34497f9cadfbb4c18ec319",
            "fe85b0dd51db4749826c7ee373aef56d",
            "de14eb7c0d7b43e29126d847d051981c",
            "4497ad1476b446509391f13bd6cbba8f",
            "0cbd232a265a4d8cbce83669cd92d1ef",
            "ee94c9f656414ae6a0b35f8813868e42",
            "cd653126f9da43b7805a03cb1872553d",
            "755aa55bb95d46e09bedb91d3c9dee03",
            "fcaff45122d946389d641ce48cedff40",
            "67eb8ee69f62464ba1a347f65d34fe75",
            "5ef99aa15d3844b4b65c905f7ca9faf9",
            "5af51a28b48c4683b04cd2933b214db4",
            "03714f3f3f4c4fc08e2f17a79cfc6aca",
            "20dbd33f0b7a40778220fada3818e77d",
            "9e60340cdb534941803fd10461d4e553",
            "78e8d54249b54639ae503554d7493166",
            "b3a707c9713e4fefa9ca01baab6063ab",
            "d49da854fe544b2cb82ff2bf36e8c760",
            "5fd68b902d4f49418a2d1795ff5d2562",
            "5f4f9242b28543c88cca496d5cb10e25",
            "37126474f24745d0bda873a8691b7f42",
            "8c0c82cbe6f94d0ba95eb087f9ff2a0f",
            "9602f0b5fc6d4a21bed5ac51c588a4da",
            "2d6c0971d7f24740ae64790a337e11ce",
            "ad1f8133d3304bf498055372fbe24fa6",
            "ec6e631ab6e34680b72c411f2ce0ffb9",
            "2a5e59f617ca45389547813581ebe5ef",
            "8a87bc8d8af5475e92f1c8571913f87a",
            "8bb0f473b61d41fdbc7cd6504cb2be2c",
            "c2bac4aeb6ef4427995f9de7f7a1112f",
            "4cd10ee7a0f34ec0ae36fde65953716a",
            "d425f715e1be44ec9c91b737879949b2",
            "b2d4a95fb605402287de62c6d9bcf635",
            "ad736ba67b7b4abaa968cd62e629a055",
            "de92d72b56994910a4dc63295d01e91c",
            "f8a1e563072649f6b0fe670377d372e9",
            "a59560422a3042bc80635c4f667cfe87",
            "05afc907584c4a6d96e240839cdedd8c",
            "ea25f54f4f4d48b69864a4a47eb6c213",
            "31d99b6f94674688948c8c210dfaa64f",
            "62565d96b44b412dacc9e356041f6cc0",
            "1703707c75134b7e85cc6208e9b36a6b",
            "773772f2009f4dc4b8baa02d779f8148",
            "e3f7862c8dad44eb8c945774cc2f58ac",
            "cea342dc18cc498084fb360eb6b811ab",
            "92a64d0cd8844d849899ae04bcb10ffd",
            "4226c42119984bf4aa191382bfe5bd0e",
            "4ac365904c8349f4a47e57599e0f9149",
            "65f5e794fed84a59bb274922ebd3ff08",
            "ed5259f7321d4c86a643cf206e48c709",
            "2d82aef1dd854468ba2e34a5a01f2f33",
            "2feca9a34aba48089bfb3d9c7122b1b8",
            "3ee01a634beb417094fcaf5b5ac40093",
            "ad2c1989e1dc4abbab422c3c59302bb8",
            "574220b8fa924f70833ce6f0aeaa3314",
            "7f694e6e9c9c4602a0f6371ea36ae1b3",
            "60bace619dd14328aeb9ded8ec316871",
            "4c1f314d292846849d5fbaee795db750",
            "6f790bd9958e49efab844debc1e1c5e8",
            "b05a41e450eb43c89316a5020b051d2f",
            "a0b40188dd964d35b3ef32fd7e84c90e",
            "a56b3a5b0c274f7dac4581858947ab07",
            "e2c2fd66408d422eb3ae33bc3d7061fb",
            "4d2214ca69044098a3e99b2978b42d9f",
            "db8e696f7687409c9971d5a67506b063",
            "1c693fce54754766a77d062638835edc",
            "06619440ed554b0594660b96abaf387e",
            "551d43f48f534afbb5fd222fc1b4ca86",
            "d5371a4090cd4f8e9dd3f005d7789c7e",
            "a26b8342222c4be18a881738b1cf4d33",
            "6ffea0d7d43241a1aed512b92efe33f8",
            "53f8bbefe4844ab4815460340ccc1852",
            "3f6638847e18424fac9ab579e3129cbd",
            "857926e6d85046b3ad6da92da1c6c9a1",
            "0c396f5f644e4111bb6235fb16de5324",
            "910d89f6f2994d4d98e313223dba4d3b",
            "70f732f2b25c40a68631c524fd97f8d3",
            "8b577a7dcecd4546bc47d88cabfa582a",
            "8a4f54f9a3f349d9a972538319226135",
            "58108cbda1af448cb18acf2fcc33ccfe",
            "cff2b129808345298d4a125cf1e5a7db",
            "64ef030de6924b41ad9371a5ca5454fe",
            "2c433089a5654b8981ef11faf843d616",
            "9c23bbb16fba48e68d82df5165cdc284",
            "c7f65efc13514dc0838c0cd5f9b324c3",
            "c668f35d1eab4418976eb880ff0bfc25",
            "493261f690634ea482040a6bf9fb7c2b",
            "9b6ab7f3c15c4cc3a67b9ebc1c714d6f",
            "716d66c01f55469ab45bb759516aa35b",
            "153a908a2bf84e9e99e632b4930af21a",
            "cb5723c4948f4b55947d5683af5a60b9",
            "6a3bbd4604b34c5587f8db29d08da070",
            "e12570c13ec14b45b9b040b22c27d65b",
            "f9d0c9d5811341ae83293c981ccddd8f",
            "f3a9b2e2506d4e6083b15adc0de1bc70",
            "eada3d01805040a28a00a0113dc5e1b4",
            "7971d261f7d6430ea44c4d0e50942043",
            "4cfe63c33c34409c8b0ac66539d5e5ae",
            "f9210d2b019d493e97e11a5221c76e35",
            "3a97bab1de204defb93e20ccde736836",
            "f0859b08834e4f93893e6d746b2dc4f2",
            "635a76f7ffb34658815646c4693f478a",
            "4dd60caa796c41968e74542d0859a226",
            "9bd3d033d56642a7b9b773f07e4e2693",
            "bd2c1cb40af64442a9207d96ea0e887d",
            "3268252b275148f08099977199be1d2a",
            "82a70d46b7524dac9c81de6da892763d",
            "88ba98f7097644b48840fb7324623e71",
            "926f05e53063421fbbdf96e8d189b864",
            "7f5c119132e74ddda04a9eb60eb562f8",
            "f7c11b6933bd46ccbbe0089b05d8dd19",
            "3cd7375e923d4145a118bd91f8ccbe07",
            "ecee03916ba04c0e83796fe3cbebf568",
            "1a0c47d612424fdf9622c9236e383863",
            "4a9c46c2cc284f7f8149ccef61e42bff",
            "29c4498715de4fcca838aed411defd53",
            "ba5cf0f1912047caaf107455a5336bea",
            "c2108d29f3974a3fbaad0b5b02b91158",
            "6e50edb30df94d6e8e03e259def389f5",
            "d436ea432ac4407bba9bfc349c05e71f",
            "01c943f8012f4410ab680630f418a5a8",
            "ac4f860658b14186b905336851d65c9a",
            "60c4d0d013ee4aac90a306f8d74c08bb",
            "693057e363d144c6aaabdfe4724922fe",
            "1301387900314b0ea5815bf53d75e13a",
            "09e9542861fa44e5b00da9e43372ddf3",
            "6181da6c7f3643059f9f24e4d7b59234",
            "d71bbabe46aa4b7abd23e0df59eca579",
            "be6fe37db568467bada110ef6e5f42a2",
            "3b71b8ad6f1c4d9fb0a5dc24327284dc",
            "207f18e860bb4d25ac49f182adf3b7cf",
            "cb032c2620ae49a4a9ac6bf6693b9904"
          ]
        },
        "id": "j9SnAHdc19ap",
        "outputId": "e45ad3f9-5336-4354-d7a5-84e58fbfe7c0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "LLAMA_REPO = \"Enoch/llama-7b-hf\"\n",
        "snapshot_download(repo_id=LLAMA_REPO, local_dir=\"./model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKQSag7f1nJC"
      },
      "source": [
        "**Dispatching the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrNGVHZhi62T"
      },
      "source": [
        "To properly quantize the model we'll need two functions.\n",
        " 1. `initialize_layerless_llama` creates a llama model without any layers, but correct weights otherwise\n",
        " 2. `load_and_dispatch_a_layer` loads a layer and inserts it into the model after the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls5meN1c4No-"
      },
      "outputs": [],
      "source": [
        "# Disabling fancy model initialization since we'll override those values anyway\n",
        "def skip(*args, **kwargs):\n",
        "    pass\n",
        "torch.nn.init.kaiming_uniform_ = skip\n",
        "torch.nn.init.uniform_ = skip\n",
        "torch.nn.init.normal_ = skip\n",
        "\n",
        "\n",
        "def initialize_layerless_llama(checkpoint_path):\n",
        "    config = LlamaConfig.from_pretrained(LLAMA_REPO)\n",
        "    config.num_hidden_layers=0\n",
        "\n",
        "    model = LlamaForCausalLM(config)\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"pytorch_model-00033-of-00033.bin\")))\n",
        "    model.seqlen = 2048\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    return model.to(torch.float16)\n",
        "\n",
        "\n",
        "def load_and_dispatch_a_layer(layer_idx, checkpoint_path, model: LlamaForCausalLM):\n",
        "    if checkpoint_path == \"TEST\":\n",
        "        linear = nn.Linear(16, 16)\n",
        "        linear.weight.data = torch.arange(16 * 16).reshape(16, 16).float()\n",
        "        model.model.layers.append(nn.ModuleDict({\"submodule\": linear}))\n",
        "        return\n",
        "\n",
        "    config = transformers.AutoConfig.from_pretrained(LLAMA_REPO)\n",
        "\n",
        "    layer = LlamaDecoderLayer(config)\n",
        "    layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"pytorch_model-{layer_idx + 1:05}-of-00033.bin\"))\n",
        "    layer_state_dict = {name[len(f\"model.layers.{layer_idx}.\"):]: tensor for name, tensor in layer_state_dict.items()}\n",
        "    layer.load_state_dict(layer_state_dict, strict=False)\n",
        "    del layer_state_dict\n",
        "\n",
        "    model.model.layers.append(layer.to(torch.float16))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsOwpAvbi62U"
      },
      "source": [
        "Calling `initialize_layerless_llama` and then calling `load_and_dispatch_a_layer` for each layer in order would fully load the model, but we'll also quantize the layes as we go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agCFfP7x2Au6"
      },
      "source": [
        "### RTN Quantization for LLaMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwS6bTIwi62V"
      },
      "source": [
        "**Auxiliary functions:**\n",
        " * `find_layers` takes a module and returns a dictionary containing all of it's *Linear* submodules with their path-names as the keys.\n",
        " * `replace_submodule` takes a module, a path-name and a submodule and replaces the module's submodule at path-name with the new submodule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8vo0UYKvi62V"
      },
      "outputs": [],
      "source": [
        "def find_layers(module: nn.Module, name='') -> dict[str, nn.Module]:\n",
        "    if type(module) == nn.Linear:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "\n",
        "def replace_submodule(module, submodule_path, new_submodule):\n",
        "    submodule_names = submodule_path.split(\".\")\n",
        "    for submodule in submodule_names[:-1]:\n",
        "        module = getattr(module, submodule)\n",
        "    setattr(module, submodule_names[-1], new_submodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJALPHCSi62W"
      },
      "source": [
        "**Load-Quantize cycle**\n",
        "Function below uses the functions above to load the layers one by one and iterate over their `Linear` submodules replacing them with `QuantizedLinear`.\n",
        " * Quantization happens on `.cuda()`, because we'll load *LLaMA* in `float16` which is not supported on `cpu`.\n",
        " * We call `torch.cuda.empty_cache()` after processing each layer because we'll have just enough *VRAM* for this to work.\n",
        " * The loaded model is placed in RAM.\n",
        "\n",
        "RTN quantization for *LLaMA*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TRwggs3Lb2F"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_rtn(checkpoint_path: os.PathLike, model: LlamaForCausalLM, bits: int):\n",
        "    \"\"\"Loads LLaMA layers one by one and quantizes them with RTN\n",
        "    Args:\n",
        "        checkpoint_path (os.PathLike): folder containing LLaMA weights\n",
        "        model (LlamaForCausalLM): model to dispatch layers into\n",
        "        bits (int): number of bits to quantize to\n",
        "    \"\"\"\n",
        "    # Load and quantize all the layers\n",
        "    layers = model.model.layers\n",
        "    assert len(layers) == 0\n",
        "    for i in trange(32):\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].cuda()\n",
        "\n",
        "        linear_submodules = find_layers(layer)\n",
        "        # Quantize the linear layers and replace the original ones with them\n",
        "        for name, linear in linear_submodules.items():\n",
        "            quantized_weight, scale, zero = measure_and_quantize(linear.weight.data, bits)\n",
        "            quantized_linear = QuantizedLinear(quantized_weight, scale, zero, linear.bias)\n",
        "            replace_submodule(layer, name, quantized_linear)\n",
        "\n",
        "        layers[i] = layer.cpu()\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f5512193cac14be79a03a91303585996",
            "8703f0d3f77c4471b6ea1654d50f79b9",
            "8d627e54749c48d4aa778710e4835402",
            "d69b22e132f949afa452cdcece065970",
            "790e330a9e5b4ce090e23eca9a392312",
            "968948ef37044cd381f1bb3d9d3eadfc",
            "eea3c27c6ff84606b30411b679b86c03",
            "3a5dcd83c58a4f478db23ba607ff409f",
            "a9ed56aebe1e48e2925fb37ed18602b0",
            "41489804798d49aabaef2f6188092d51",
            "fc7eec47f30f4ee4affe159b58444f21"
          ]
        },
        "id": "wj8FBs1ji62W",
        "outputId": "c9713e32-0ea3-4388-d0fd-aa33b133b72f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5512193cac14be79a03a91303585996",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "model = nn.ModuleDict({\"model\": nn.ModuleDict({\"layers\": nn.ModuleList([])})})\n",
        "llama_rtn(\"TEST\", model, 4)\n",
        "\n",
        "assert len(model.model.layers) == 32, \"You didn't load all the layers\"\n",
        "assert all(isinstance(layer.submodule, QuantizedLinear) for layer in model.model.layers), \"Some Linears weren't properly replaced\"\n",
        "assert torch.all(model.model.layers[0].submodule.quantized_weight == torch.arange(16).unsqueeze(0).repeat(16, 1)), \"Quantized weights are weird\"\n",
        "assert torch.all(model.model.layers[0].submodule.scale == 1), \"Quantized scales are weird\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4OlcGF2aEts"
      },
      "source": [
        "### Testing the Quantized Model\n",
        "\n",
        "Now we have everything we need to quantize the _LLaMA-7B_ model to 4 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "luUgnOhxaFcw"
      },
      "outputs": [],
      "source": [
        "MODEL = \"./model/\"\n",
        "SEED = 0\n",
        "BITS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "327f6f1b242e473d8885af6c6ac7f599",
            "ddfcd24f32a4466988eb777250623219",
            "bb9c5313fdac4f289fda56c16209e111",
            "4185e3e74925438595743f5a760257a2",
            "996d296d606c4320b2c865557b894ceb",
            "45b1cc2873674f7abdddebe3c7cfb764",
            "7a7d6e86058d463888cc0257482891b8",
            "9767da8ac62446f299a1c6b9e9c4c382",
            "2518d6c1b9234d4496a29707fa515511",
            "dfa091d0a9e245f18b2e183ed8d95523",
            "3539e8375f1e48ca8947330286b0fc71"
          ]
        },
        "id": "tUD4z4t7a7z8",
        "outputId": "7da0b764-4ab2-4bd8-ccbf-cec0262ebb68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "<ipython-input-10-18d87c488859>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"pytorch_model-00033-of-00033.bin\")))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "327f6f1b242e473d8885af6c6ac7f599",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-18d87c488859>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"pytorch_model-{layer_idx + 1:05}-of-00033.bin\"))\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n",
        "\n",
        "model = initialize_layerless_llama(MODEL)\n",
        "llama_rtn(MODEL, model, BITS)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "3a8fa2bae3964c138974e4808d84bbfd",
            "e7da7e4996614e56aca90a7f3a9ebdd2",
            "5b4f6b9d8781436ea4f9032c83cb411a",
            "b6a84e610627469086503b33ed1d9638",
            "e248311536004e6aaa949b1d774a37f3",
            "a25531fae6884f328e8bdecd41d9ff18",
            "8a782a1c6d9643deb25562265638bc77",
            "f55ce3d54c724f798f60f32ed5d8aa44",
            "a5e66be8de6b40e1bdccab4bb087d1cf",
            "9acfbc0bd934436396b6eea0f5090c3f",
            "2c236094260947ee892976c2624f99a7"
          ]
        },
        "id": "8EN9tVawbUfG",
        "outputId": "34ee4c92-8d5a-4536-a05f-54d8a385037e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a8fa2bae3964c138974e4808d84bbfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you explain the Pythagorean theorem?\",\n",
        "    \"What is photosynthesis?\",\n",
        "    \"Give me a summary of 'Romeo and Juliet'\",\n",
        "    \"How far is the moon from the Earth?\",\n",
        "    \"What is a haiku?\",\n",
        "]\n",
        "answers = []\n",
        "\n",
        "for question in tqdm(questions):\n",
        "    tokenized_input = tokenizer(\n",
        "        f\"QUESTION: {question}\\n ANSWER:\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            tokenized_input[\"input_ids\"].cuda(),\n",
        "            max_length=50, num_beams=3, early_stopping=True,\n",
        "        )[0]\n",
        "    answer = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    answers.append(answer[:answer.find(\".\")] + \".\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-QFd87GjNvd",
        "outputId": "248735c1-fd13-42fe-b465-e7334cdaafb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: What is the capital of France?\n",
            " ANSWER: Paris.\n",
            "\n",
            "QUESTION: Can you explain the Pythagorean theorem?\n",
            " ANSWER: The Pythagorean theorem states that the sum of the squares of the sides of a right triangle is equal to the square o.\n",
            "\n",
            "QUESTION: What is photosynthesis?\n",
            " ANSWER: Photosynthesis is the process by which plants use the energy of sunlight to convert carbon dioxide and water into carbohydrates an.\n",
            "\n",
            "QUESTION: Give me a summary of 'Romeo and Juliet'\n",
            " ANSWER: Romeo and Juliet is a tragedy written by William Shakespeare about two young star-crossed lovers, Rom.\n",
            "\n",
            "QUESTION: How far is the moon from the Earth?\n",
            " ANSWER: About 240,000 miles.\n",
            "\n",
            "QUESTION: What is a haiku?\n",
            " ANSWER: A haiku is a form of Japanese poetry that consists of three unrhymed lines of 5, 7, and 5 syllables.\n"
          ]
        }
      ],
      "source": [
        "print(*answers, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Fn-Vko3urrM0"
      },
      "outputs": [],
      "source": [
        "model = model.cpu()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFpjGxX2H-J"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "Before we start quantizing the model itself, let us create a way to evaluate it's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6Muc5ZyupZ"
      },
      "source": [
        "**Downloading the data**\n",
        "\n",
        "As a metric of the models' performance, we'll use it's PPL on the [wikitext2](https://paperswithcode.com/dataset/wikitext-2) dataset. Let's download and tokenize it. We'll need two subsets of it:\n",
        " * Test set to evaluate the models.\n",
        " * A train subset that we'll use later for GPTQ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "556c6d19439f4811aa02ec72c5100164",
            "933391c1accd42a4b55319f862612ecf",
            "ad49db5d78d14b4aa25d78858de120b0",
            "6c059fbc111c4f03a87d435a400a0006",
            "372584da997a46c196456ebcf7e02bdf",
            "aa75ea0968ce430c9b0d62be635c2467",
            "61a324e9381f41ed952ae5ce2b20b5ce",
            "b41b9cc21dd84e8590dfe4561310983c",
            "6e7be0ab129747d6b63a8d06b47239dd",
            "1b5e639d2f81411eae805b3916d87e87",
            "fb01d20813fa419aaeb06751a440eea6",
            "1fc1264c1bad4490bfd69587be0817e3",
            "0f7f6a02b7dc4622b1bff343000d7734",
            "88f1be29df54454e833ee926aad502e0",
            "d735b2534ac148e6a95fe0024c819d5f",
            "afb990fea3194f8bb7eb7b6fe34a95be",
            "f4da901dae4841ccb0665c67050888c1",
            "f7f78ea2f6b544eba95b9e911f196c1e",
            "e65a35edffc24658a59f43da51a78879",
            "1401c4de5b604b3297f57ed85a278eda",
            "3b3fc8b232554cfc9c06ecc6db393f55",
            "3eea857f25f542629e9d853b4a99cda1",
            "41ba34d95a5945a38629d52f0d82778d",
            "73b7bf76c368401c9a656c6989866914",
            "dfb5403991054bee8cf5fb707c318c02",
            "fdb83a4ab35d4762900bda3377fa64a5",
            "bfacd1cb9c034fd6a8cdbca4fca01cd7",
            "a1501496e9f54e40ae0545594314e692",
            "592f0d037fdb4655865c8403b16ceca7",
            "5ea992a479ff4913bee1e0919881f168",
            "c274b8b718984b3080cf0e9c5db82842",
            "b2e94a85baf64008bb4390b20de88256",
            "ae8935aaeab747c79092721ae0706a4c",
            "c65bf650abd447f69a746ae50ce58e0f",
            "b20b3b181dde43768b8310b03dc99393",
            "48bc4455815e4ab39c9fed4b2b4a2238",
            "b3a442b2592b4ee6aedc7195fce01b4c",
            "f02d0abac9ea4dbdb11979ed643c521b",
            "641f8c039b2a415e81df96727b9ec383",
            "74b08a7bfc284f5fa56dbc87a9896238",
            "03ec316b4e1f4f55a64e5714e112c48d",
            "b3344d27fd434371982aa828be2f4cb6",
            "99f197c35c58458e88ca53b7c21c1db8",
            "29f13ce886ff4ee09217d81e6d5de917",
            "464c07ff41b64706a07d1ff335d1fa2f",
            "c1e763a15de24623981baf5f4e2ae49a",
            "fbb9e82b89134ea98a200391e2f5e3c7",
            "91da07d161b6454c9825bb17da632bdd",
            "b5810b5c4614428bb4d1fcd8886f3636",
            "e4f2e787fb5d4b95a39226e2cf97fd9e",
            "cb01b97b4e65493ebb4b746f823e1e72",
            "147a8af192ab41d9881fdaac5b7f1cc3",
            "cb3bd7d54b484efab521c853986147a5",
            "d1aefa5cc3f4478894a0fa147d47de5c",
            "8275835c6eb14aef9b3c8e7a844ddf4d",
            "75d88d79da634a728715507d8fbb5d83",
            "699e7d11d3cc45aaa349cf84f5dd5c48",
            "5833712afea04a86b5d9af17fa40570e",
            "06918a47ff374a3d94eb523910e2d80a",
            "04209a7a16fd4a5d8d35bc22bf37fa35",
            "d18eedcc340f4a92b00a2e87f93cda6d",
            "fc54e2dd9e51452196cbae8d232f2e5a",
            "de5bbe8d4504488f816b2a7894eccebe",
            "16b675c2f02341dfb533d8760ea6a8cc",
            "f118f54bdad447128660d90a79e38e8b",
            "58e63b3250514cb2970016ee0c06ca35",
            "274eb25df146488da82dfcacaec82032",
            "69bd151423cd459ba64cd696334970ba",
            "7c09e6813f2241d1b0893bc338217c5c",
            "de6021aec3174ebcbf9db1ba9282ad43",
            "de4e9606e1ec499aa091df5dddbf8196",
            "e98a84f7e28146ab9305675215e26715",
            "4efca49c31434f458ca1cea07d4105f8",
            "df34e0cec9f84cdcb21852c135e69757",
            "8e8012fc0662422abfdc26368ca8f243",
            "feca0c99b18142c4903c7fff24c7bbc7",
            "db57e7c3e5ff44dd8cdc3f9e8181abe8",
            "8b2d40b4c4054f41b55af442afe8d23b",
            "42ea8d8366e140bb9975540770bee9d9",
            "d8cede5434fe4428814d8349c76fc025",
            "d6d66213ef9140209beb20fc4bb52c4b",
            "54e06e95f1214dda96466ed4feec2ec6",
            "79866189d8cf488cba35c1944ce2ae86",
            "2c8797d9868140b2bb1e836ea0d175d6",
            "37ceabbdabd44f3b8943098d1d91815f",
            "71ebd4004dd143c3acf58428ce28745f",
            "e6c575ec9d6b4b0582f59048a7527772",
            "c072f2c6788149c28cd303739a33f480",
            "ee07ace4fbed4ddaa90773baa0068d71",
            "445e4de06d6b4757b7e642d69af85a16",
            "2a073e88a0b64dbd90112bc70b950d98",
            "1c57c5b0d86a48539ea0e916f8dbe3f9",
            "27847269b16a4092a337f37799da1ba1",
            "95053bf33a574909b516fb659e3453b0",
            "12084071e522417b94cd51eaaabadee5",
            "12e66679bf55497aa17e4c0844851614",
            "a008a1c2fbed42d3a7667e5920384002",
            "626b5dfe76014ab9bef4db8dccee79d4",
            "982bd5135f3b408ba89d7f7b55c33a6a"
          ]
        },
        "id": "CUSNnguPysaD",
        "outputId": "a5a5fcd5-aaec-4262-a10d-d57a2eb29f08"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "556c6d19439f4811aa02ec72c5100164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fc1264c1bad4490bfd69587be0817e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41ba34d95a5945a38629d52f0d82778d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c65bf650abd447f69a746ae50ce58e0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "464c07ff41b64706a07d1ff335d1fa2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75d88d79da634a728715507d8fbb5d83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "274eb25df146488da82dfcacaec82032",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b2d40b4c4054f41b55af442afe8d23b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee07ace4fbed4ddaa90773baa0068d71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SEED = 0\n",
        "\n",
        "def get_wikitext2(seed, seqlen, nsamples=128):\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n",
        "\n",
        "    train_input_ids = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt').input_ids\n",
        "    random.seed(seed)\n",
        "    train_batch = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, train_input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = train_input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        train_batch.append(inp[0])\n",
        "\n",
        "    test_input_ids = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt').input_ids\n",
        "    test_input_ids = test_input_ids[:, :(test_input_ids.shape[1] // seqlen) *  seqlen]\n",
        "    test_input_ids = test_input_ids.reshape(test_input_ids.shape[1] // seqlen, seqlen)\n",
        "\n",
        "    return torch.stack(train_batch), test_input_ids\n",
        "\n",
        "train_batch, test_input_ids = get_wikitext2(SEED, 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77M_WBTuOLh"
      },
      "source": [
        "**Model offloading**\n",
        "\n",
        "We want to evaluate the model's performance on a large dataset. The model barely fits on the *GPU*, and we'll have to infer in on long text sequences. We don't have enought *VRAM* to do that.\n",
        "\n",
        "Instead, we'll keep most of the model in *RAM*, only transferring the layers to *GPU* as we go through them one by one, and putting them back when we're done.\n",
        "\n",
        "**Obtaining the first layer inputs**\n",
        "\n",
        "To start iterating over the layers, we'll first have to obtain the fist layer inputs. We use the function below to do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbxZfwj8ipPF",
        "outputId": "5a05f8f0-e47e-4a81-dac6-a28c59bd3808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-01 10:43:56--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1334 (1.3K) [text/plain]\n",
            "Saving to: \u2018utils.py\u2019\n",
            "\n",
            "utils.py            100%[===================>]   1.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 10:43:56 (78.8 MB/s) - \u2018utils.py\u2019 saved [1334/1334]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/utils.py --no-check-certificate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7W50rdH0i62Y"
      },
      "outputs": [],
      "source": [
        "from utils import get_first_layer_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "2e420167b0e7421f8e56ef40d526956b",
            "800cb70ccd804394a394a1d9f1ea79c3",
            "b411b23eb5694f14bae286461a7c5496",
            "69a9b5d6721346468516b7c9d4bafb57",
            "c98156bf80874eeb8a385d9f0233c51f",
            "572e434738cc443fb4de0a0221983626",
            "85490ee7e2344707ae6bba8380b2d069",
            "70de15daad5e40d9b5b9f79e9d261c09",
            "0697d191d4ca4bfa877ea2611f3fc44a",
            "a3e9b3588d1c4198ade8db9c23438853",
            "c932b04bee3940e8b0b2c440d72ebc08"
          ]
        },
        "id": "hN2TrGEBi62Y",
        "outputId": "3d5fa53b-a345-4560-efff-ce9e64ad9bee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e420167b0e7421f8e56ef40d526956b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "class TestModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleDict({\"layers\": nn.ModuleList([])})\n",
        "\n",
        "    def forward(self, inp):\n",
        "        self.model.layers[0](2 * inp, attention_mask=\"Some Mask\", position_ids=\"Some Ids\")\n",
        "\n",
        "\n",
        "test_model = TestModule()\n",
        "llama_rtn(\"TEST\", test_model, 4)\n",
        "test_inputs = torch.arange(16 * 32).reshape(16, 32).float()\n",
        "hidden_states, attention_mask, position_ids = get_first_layer_inputs(test_model, test_inputs)\n",
        "assert torch.all(2 * test_inputs == hidden_states)\n",
        "assert attention_mask == \"Some Mask\"\n",
        "assert position_ids == \"Some Ids\"\n",
        "assert len(test_model.model.layers) == 32, \"The function doesn't put back the original layers\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFI_nTCXi62Y"
      },
      "source": [
        "**Forward passing layer-by-layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zzEPMSsEi62Y"
      },
      "outputs": [],
      "source": [
        "def forward_pass_layer(layer: nn.Module, inps: torch.Tensor, outs: torch.Tensor, attention_mask: Tensor, position_ids: Tensor):\n",
        "    \"\"\"Forward pass inps through the layer ONE INP AT A TIME saving the outputs into the corresponding elements of outs\"\"\"\n",
        "    for j in range(inps.shape[0]):\n",
        "        outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "\n",
        "\n",
        "def get_batch_nll(model: nn.Module, batch: Tensor):\n",
        "    # Collect the first layer inputs, put them on .cuda()\n",
        "    inps, attention_mask, position_ids = get_first_layer_inputs(model, batch)\n",
        "    inps = inps.cuda()\n",
        "    attention_mask = attention_mask.cuda()\n",
        "    position_ids = position_ids.cuda()\n",
        "\n",
        "    # Create a buffer for layer outputs\n",
        "    outs = torch.zeros_like(inps)\n",
        "\n",
        "    # Forward pass through the layers\n",
        "    layers = model.model.layers\n",
        "    assert len(layers) == 32\n",
        "    for i in trange(32, leave=False):\n",
        "        layer = layers[i].cuda() # Take a layer and put in on .cuda()\n",
        "\n",
        "        forward_pass_layer(layer, inps, outs, attention_mask, position_ids) # Forward pass a layer\n",
        "        inps, outs = outs, inps # Prepare the inputs and the output buffer for the next layer. Reuse the existing buffers\n",
        "\n",
        "        layers[i] = layer.cpu() # Put layer back on .cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # Calculate NLL\n",
        "    nll = 0\n",
        "    model.model.norm = model.model.norm.cuda()\n",
        "    model.lm_head = model.lm_head.cuda()\n",
        "    for i in range(inps.shape[0]):\n",
        "        lm_logits = model.lm_head(model.model.norm(inps[i].unsqueeze(0)))\n",
        "        labels = batch[i]\n",
        "        # Calculate the language modeling Negative Log Likelyhood\n",
        "        shift_logits = lm_logits[:, :-1, :]\n",
        "        shift_labels = labels[1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).cuda())\n",
        "        nll += float(loss) * model.seqlen\n",
        "    model.model.norm = model.model.norm.cpu()\n",
        "    model.lm_head = model.lm_head.cpu()\n",
        "    return nll\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, test_input_ids):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    total_nll = 0\n",
        "    for batch in tqdm(torch.tensor_split(test_input_ids, 4)):\n",
        "        total_nll += get_batch_nll(model, batch)\n",
        "\n",
        "    # Calculate PPL\n",
        "    ppl = math.exp(total_nll / test_input_ids.numel())\n",
        "    print(f\"PPL: {ppl}\")\n",
        "    return ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHq3Z8RlLpuc"
      },
      "source": [
        "**Calculating PPL**\n",
        "\n",
        "We've already loaded and quantized the model. All that's left is to evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9fe47144ca3e4fb69a85cc02538ff4ff",
            "f9440c9ea4ca47438c65649ba878c12b",
            "acc8fe1da90b4282b8b33032d6be3749",
            "d8232341d2f34a1aae84c7c071aac7f1",
            "515c6cbfd8024122bbc1640c653753dd",
            "dfc26531b98748aa911677be6ed366b1",
            "36e63336b8f6436db80b33cc8f3d2fa5",
            "0ad9b397d5354c7a8ac38a1beca46c65",
            "088b6921fe4d42569cc849030ade4648",
            "d35bc91b8a4d4bdeb1b98985380d2192",
            "ead5a3ac19cc42c09733ba32714b6481",
            "ac301c4ea77f4372a969657e6cd23084",
            "2e2aa7e62fac492f8bad1adbc32df93f",
            "85c18a6e05a34b8881f8d102b8d8a25e",
            "20852d8eddc2415884ce877e8a809b8e",
            "070cc0c728de47cda855f5a3cc384873",
            "5f4e3cdddcfc49369ba11fb2a5e32e3a",
            "cd00cb74290f4f0d8aeaba578069cd62",
            "ae2b518b327749668e68ec04c675a3bf",
            "bfcd72a5795a4a0fabbc9bf17c0a056a",
            "6759ccaa73aa41de82e4298989c9a594",
            "b9d8694265bc486b8493c2718283d5b4",
            "4ebd71bb3a4a4058909ad5e2a441c0b6",
            "d6b6eff16b9240b78ead53ff5ab9683d",
            "f3200161ed3a4704823d7040e9d6a20f",
            "3fcd8ea4c10545788a98bc9ed7827195",
            "77697f06d7344af1a3dbad612261b41f",
            "0be47180745345e884855399d1a61161",
            "ca16e05a7ff543968aeb3402ab85f39d",
            "c01f15dcedb04e868974ddd0f5965226",
            "1350606280744c58bd79911193f189a1",
            "1aaa8b373ac54b43a841b591101939f9",
            "998cd684484d4c04ab350b13a70fcbb5",
            "3cd486cab91142378d6d4bb013b4b62c",
            "4d89c2be3ed54098a77e1c8a8ba0f32c",
            "3cdd178090ca432aa9b23644b6cb9785",
            "1215e19128d14a8fb16441ce88d79b14",
            "8ce5697b20fb4c0c909b5bdcedb97915",
            "b55c75b617464bf388c300d5385253b5",
            "77b30a6257604eeda0b8eeb4a23163ea",
            "15e741ef85da4629a2f084a1f0435c7b",
            "4b74f9f8bd404b0e93132f2fa6c5bf62",
            "bf17848563d946cd964241b71e631c7e",
            "183a74041ec44f61940c6ac3f47f083a",
            "29f19b0f429748c28784f99518c6ceff",
            "525834828cd6455a85dc7e4d2334c076",
            "7e1e564ade8c4b428286466fb88de6be",
            "4929c061ff99481695383464e7f773bf",
            "fd5f78ccb8b84095b103b80417324dab",
            "753fa8f83db64f5e90ddb9cbeafafae3",
            "eb68cf1d876b4d07a2c56b4418720880",
            "1ad3f934c49947b9aad645c07c848a72",
            "5cf436da75804ed2923e2c46b1fb7bd3",
            "c45d21b4d2e24d019f7f72897dd5f657",
            "3cce687f29764d19859e48f0b22ef352"
          ]
        },
        "id": "ybS4iAjsq67Z",
        "outputId": "b5fe359d-678d-4455-d033-52f66cc199e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fe47144ca3e4fb69a85cc02538ff4ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac301c4ea77f4372a969657e6cd23084",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ebd71bb3a4a4058909ad5e2a441c0b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cd486cab91142378d6d4bb013b4b62c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29f19b0f429748c28784f99518c6ceff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPL: 6.429625677428965\n"
          ]
        }
      ],
      "source": [
        "rtn_ppl = llama_eval(model, test_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "722oBDnwq_1x",
        "outputId": "9a2f0ad3-4679-40f9-91de-a0fcefc04a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "assert rtn_ppl > 6.3 and rtn_ppl < 6.6\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "urb_7OhSq_zA"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQIk33arN6A"
      },
      "source": [
        "### GPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16SaWf3rQt7"
      },
      "source": [
        "GPTQ is the State Of The Art quantization algorithm for post-training DL model quantization. It works by sequentially quantizing the model's linear layer weights.\n",
        "\n",
        "Although in outputs results similar to what one would get with Round To Nearest quantization, it makes a key observations to boost it's end performance:\n",
        " * It is layer input aware (also referred to as \"1-Shot\" method), meaning int optimizes the quantized matrix to show best performance on inputs typically encountered in that layer.\n",
        "More formally, the problem can be formulated as:\n",
        "$$\n",
        "W_q = argmin_{\\widehat{W}}\\|XW^T - X\\widehat{W}^T\\|_2^2\n",
        "$$\n",
        ", where\n",
        " * $X$ is the input matrix of shape `(..., IN)`.\n",
        " * $XW^T$ is the unquantized output of shape `(..., OUT)`. We think of the norm above as taking a sum over those (...) dimensions.\n",
        " * $W$ is the unquantized weight of shape `(OUT, IN)`.\n",
        " * $\\widehat{W}$ is the quantized weight taken from some quantization grid.\n",
        "\n",
        "One can notice that the expression above is independent with regard to the rows of $W$ and $\\widehat{W}$, meaning we can solve it for each row in parallel. This is the reason why we're working with row-wise quantization in the first place. Notice that the quantization grid only depends on min/max values withing the row and not the quantization process, so we can think of it as fixed.\n",
        "\n",
        "and the dimension of the optimization problem is `IN`, which is too much to solve exactly. The algorithm proposes to solve it iteratively.\n",
        "\n",
        "Less us consider a vector of full precision weights $F$ and corresponding sent of inputs $X_F$. The corresponding objective is quadratic with Hessian\n",
        "$$\n",
        "H_F = 2X_F^TX_F^.\n",
        "$$\n",
        "The algorithm can be described like this:\n",
        " * Do the following steps until $F$ is fully quantized:\n",
        "    1. Given the next index to quantize $i$, and corresponding unquantized element $F_i$.\n",
        "    2. Quantize the coordinate by prjecting in onto the quantization grid $Q_i = quant(F_i)$.\n",
        "    3. Update all of the remaining weights $F_: = F_: - \\frac{F_i - quant(F_i)}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,:}$.\n",
        "    4. Exclude $i$ from $F$.\n",
        "\n",
        "It uses the inverse Hessian to slightly tune the remaining unquantized weights to mitigate the quantization error.\n",
        "\n",
        "As for how $i$ is chosen, an observation was made that iterating over indices in order of **decreasing diagonal Hessian elements** provides the best performance.\n",
        "\n",
        "There are a few more ideas that make this algorithm much faster:\n",
        " 1. We can represent the order of quantization (selection of $i$) by permuting the row in advance, and then iterating over the row element in order.\n",
        " $$\n",
        "   F_{i:} = F_{i:} - \\frac{F_{i} - quant(F_{i})}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,i:}\n",
        " $$\n",
        " 2. The problem is row-wise independent, meaning that we use the same permutation each row and perform those operations in a vector fashion for all the rows at the same time.\n",
        " $$\n",
        "   F_{:,i:} = F_{:,i:} - \\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\odot\\left[H_F^{-1}\\right]_{i,i:}\n",
        " $$\n",
        " 3. We don't actually need to recompute the inverse Hessian. At $i$-th step we only need its $t$-th row, and we can use fancy math to precompute the matrix containing all of those rows in advance.\n",
        " $$\n",
        "  H^{-1} = Cholesky(H^{-1})^T    \n",
        " $$\n",
        "\n",
        " 4. We don't need to tune all the remaining unquantized values right away. We can only apply the updates for the closest elements right away and accumulate all the other updates to apply them only once in a while.\n",
        "\n",
        "    We'll do this in block of fixed size, applying the updates inside of those blocks and updating the weights outside only when we're done with the block. To accumulate those updates, we'll collect the scaled quantization error\n",
        "    $$\n",
        "      Err_{:,i} =\\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\text{ for all }i\\text{ in block}.\n",
        "    $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogTj9Op5raaD"
      },
      "source": [
        "**GPTQ within blocks**\n",
        "\n",
        "Implement GPTQ within the block. Iterate over the columns in ordered vector fashion, quantizing them one by one and updating all the remaining colums within the block.\n",
        "\n",
        "Return the quantized weight as well as the matrix of quantization errors that we'll need to tune the unquantized weights outside of the block.\n",
        "\n",
        "Implementation of GPTQ block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8GwQU5CrNv_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def gptq_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Performs GPTQ within block\n",
        "    Args:\n",
        "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
        "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
        "        scale (Tensor): row-wise quantization constants of shape (OUT, 1)\n",
        "        zero (Tensor): row-wise quantization constants of shape (OUT, 1)\n",
        "        bits (int): number of bits to quantize() to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
        "    \"\"\"\n",
        "    block_weight = block_weight.clone()\n",
        "    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.uint8, device=block_weight.device)\n",
        "    scaled_block_error = torch.zeros_like(block_weight)\n",
        "\n",
        "    # Interate over the block's columns\n",
        "    for i in range(block_weight.shape[1]):\n",
        "        # Get the column and the corresponding inverse Hessian\n",
        "        column_weight = block_weight[:, [i]]\n",
        "        hessian_row = block_hessian_inverse[[i], i + 1:]\n",
        "        quantized_block_weight[:, [i]] = quantize(column_weight, scale, zero, bits)\n",
        "        scaled_block_error[:, [i]] = (column_weight - dequantize(quantized_block_weight[:, [i]], scale, zero)) / block_hessian_inverse[i, i]\n",
        "\n",
        "        block_weight[:, i + 1:] -= scaled_block_error[:, [i]] @ hessian_row\n",
        "\n",
        "    return quantized_block_weight, scaled_block_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgRy6B_didyY",
        "outputId": "0e04493d-7ad7-44db-96fc-3cfe69d1162c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-01 11:21:19--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_weight_reference.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3392 (3.3K) [application/octet-stream]\n",
            "Saving to: \u2018gptq_block_weight_reference.pt\u2019\n",
            "\n",
            "\r          gptq_bloc   0%[                    ]       0  --.-KB/s               \rgptq_block_weight_r 100%[===================>]   3.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 11:21:19 (47.9 MB/s) - \u2018gptq_block_weight_reference.pt\u2019 saved [3392/3392]\n",
            "\n",
            "--2024-12-01 11:21:20--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_error_reference.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9531 (9.3K) [application/octet-stream]\n",
            "Saving to: \u2018gptq_block_error_reference.pt\u2019\n",
            "\n",
            "gptq_block_error_re 100%[===================>]   9.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 11:21:20 (90.5 MB/s) - \u2018gptq_block_error_reference.pt\u2019 saved [9531/9531]\n",
            "\n",
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "!wget -O gptq_block_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_weight_reference.pt --no-check-certificate\n",
        "!wget -O gptq_block_error_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_error_reference.pt --no-check-certificate\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(0)\n",
        "\n",
        "weight = torch.rand((128, 128), generator=generator).cuda()\n",
        "scale, zero = get_scale_and_zero(weight, 15)\n",
        "\n",
        "block_weight = weight[:,:16]\n",
        "\n",
        "block_hessian_inverse = (torch.triu(torch.rand((16, 16), generator=generator), diagonal=1) + torch.diag(torch.rand(16, generator=generator) + 1)).cuda()\n",
        "quantized_block_weight, scaled_block_error = gptq_block(block_weight, block_hessian_inverse, scale, zero, 4)\n",
        "\n",
        "assert torch.all(quantized_block_weight == torch.load(\"gptq_block_weight_reference.pt\", weights_only=True))\n",
        "assert torch.allclose(scaled_block_error, torch.load(\"gptq_block_error_reference.pt\", weights_only=True), rtol=1e-5, atol=1e-06)\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXmzbg8y18R"
      },
      "source": [
        "**Now we can implement the full algorithm:**\n",
        " * Get row-wise quantization constants.\n",
        " * Sort the columns by decreasing Hessian diagonal values. Think about how you'd have to permute the Hessian as well.\n",
        " * Process the Hessian to obtain the precomputed inverse Hessian.\n",
        " * Iterate over the columns in blocks:\n",
        "    * Get the next block and quantize it.\n",
        "    * Tune all the following blocks to mitigate the quantization error.\n",
        "      $$\n",
        "         F_{:,block\\_end:} = F_{:,block\\_end:} - Err_{:,block\\_start:block\\_end}\\odot\\left[H_F^{-1}\\right]_{block\\_start:block\\_end,block\\_end:}\n",
        "      $$\n",
        " * Restore the original order for quantized columns.\n",
        "\n",
        "Implementation of the full algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqI4FamprFPE"
      },
      "outputs": [],
      "source": [
        "def prepare_inverse_hessian(hessian: Tensor, percdamp: float) -> Tensor:\n",
        "    \"\"\"Precomputes inverse Hessian\n",
        "    Args:\n",
        "        hessian (Tensor): problem hessian\n",
        "        percdamp (float): diagonal damping constant for numerical stability\n",
        "    Returns:\n",
        "        Tensor: precomputed inverse Hessian\n",
        "    \"\"\"\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(hessian.shape[0], device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    return hessian\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def gptq(weight: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Quantizes weight with GPTQ\n",
        "    Args:\n",
        "        weight (torch.Tensor): weight to quantize\n",
        "        bits (int): number of bits to quantize to\n",
        "        hessian (torch.Tensor): problem Hessian\n",
        "        blocksize (int, optional): Defaults to 128.\n",
        "        percdamp (float, optional): Hessian damping constant for numerical stability. Defaults to .01.\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized_weight, row-wise quantization scales, row-wise quantization zeroes\n",
        "    \"\"\"\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "    num_columns = weight.shape[1]\n",
        "    hessian = hessian.float()\n",
        "\n",
        "    # Identify and patch always-zero input coordinates\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Get row-wise quantization constants\n",
        "    scale, zero = get_scale_and_zero(weight, 2 ** bits - 1)\n",
        "\n",
        "    # Sort the columns by decreasing Hessian diagonal values.\n",
        "    # Transform the hessian accordingly.\n",
        "    perm = torch.argsort(torch.diag(hessian), descending=True)\n",
        "    invperm = torch.argsort(perm)\n",
        "    weight = weight[:, perm]\n",
        "    hessian = hessian[perm][:, perm]\n",
        "\n",
        "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
        "    hessian_inverse = prepare_inverse_hessian(hessian, percdamp)\n",
        "\n",
        "    # Iterate over the columns in blocks\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.uint8, device=weight.device)\n",
        "    for block_start in range(0, num_columns, blocksize):\n",
        "\n",
        "        block_end = min(block_start + blocksize, num_columns)\n",
        "\n",
        "        # Get the next block and quantize it\n",
        "        block_weight = weight[:, block_start:block_end]\n",
        "        block_hessian_inverse = hessian_inverse[block_start:block_end, block_start:block_end]\n",
        "        quantized_block_weight, scaled_block_error = gptq_block(block_weight, block_hessian_inverse, scale, zero, bits)\n",
        "        quantized_weight[:, block_start:block_end] = quantized_block_weight\n",
        "        # Tune all the following blocks to mitigate the quantization error\n",
        "        weight[:, block_end:] -= scaled_block_error @ hessian_inverse[block_start:block_end, block_end:]\n",
        "\n",
        "    # Reverse the permutation of the quantized weight\n",
        "    quantized_weight = quantized_weight[:, invperm]\n",
        "\n",
        "    return quantized_weight, scale.to(dtype), zero.to(dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI1R_stdrFMk",
        "outputId": "1dde0e75-bc11-49ce-b92a-fd1ee2e1a21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-01 11:46:58--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_weight_reference.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17634 (17K) [application/octet-stream]\n",
            "Saving to: \u2018gptq_weight_reference.pt\u2019\n",
            "\n",
            "\rgptq_weight_referen   0%[                    ]       0  --.-KB/s               \rgptq_weight_referen 100%[===================>]  17.22K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-12-01 11:46:58 (12.7 MB/s) - \u2018gptq_weight_reference.pt\u2019 saved [17634/17634]\n",
            "\n",
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "!wget -O gptq_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_weight_reference.pt --no-check-certificate\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(0)\n",
        "\n",
        "hessian = torch.triu(torch.rand((128, 128), generator=generator) + 4 * torch.eye(128)).cuda().half()\n",
        "hessian += hessian.clone().T\n",
        "quantized_weight, _, _ = gptq(weight, 4, hessian, 32)\n",
        "\n",
        "assert torch.all(quantized_weight == torch.load(\"gptq_weight_reference.pt\", weights_only=True))\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHAnRizBy8HC"
      },
      "source": [
        "**Sequential Model Quantization**\n",
        "\n",
        "The GPT quantization approach implemented here requires an ordered approach due to its input-dependent nature. For each `Linear` submodule within the GPT model, we need to ensure that the input data is representative of the actual operating conditions post-quantization. This involves propagating a batch of input samples through the model sequentially, with each layer's input being the output of the preceding **quantized** layers.\n",
        "\n",
        "The quantization process must follow a strict sequence both across and within layers. Within each layer, there is a predetermined order in which the submodules must be quantized, which is dictated by the dependencies between them. This order is defined by the \"sequential groups\".\n",
        "\n",
        "The steps of the algorithm are as follows:\n",
        "- Retrieve and prepare inputs for the first layer.\n",
        "- Iterate through each layer in the model:\n",
        "  - Load the current layer for processing.\n",
        "  - Within each layer, process the sequential groups of submodules:\n",
        "    - Attach forward hooks to collect input data to each submodule.\n",
        "    - Execute a forward pass through the layer to accumulate the necessary input data for quantization.\n",
        "    - Remove the hooks after data collection.\n",
        "    - Apply GPTQ to quantize the submodule weights using the accumulated input data.\n",
        "  - Perform another forward pass through the quantized layer to generate the inputs for the next layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMyrGkHqy-xg"
      },
      "outputs": [],
      "source": [
        "def get_accumulate_input_fn(name: str, hessians: Mapping[str, Tensor], num_samples: Mapping[str, int]):\n",
        "    \"\"\"Generates a callback that updates the corresponding hessians and counts when given input\n",
        "    Args:\n",
        "        name (str): module name\n",
        "        hessians (Mapping[str, Tensor]): a dict of modules' hessians, accessible by module name\n",
        "        num_samples (Mapping[str, int]): a dict of callback call counters\n",
        "    \"\"\"\n",
        "    def tmp(_, inp, out):\n",
        "        inp = inp[0].data # ... x hidden_size\n",
        "        inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "        inp = inp.t().float() # hidden_size x inputs\n",
        "        num_samples[name] += 1\n",
        "        if hessians[name] is None:\n",
        "            hessians[name] = inp.matmul(inp.t())\n",
        "        else:\n",
        "            hessians[name] += inp.matmul(inp.t())\n",
        "    return tmp\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_gptq(checkpoint_path: os.PathLike, model: LlamaForCausalLM, batch: Tensor, bits: int):\n",
        "    \"\"\"Loads LLaMA layers one by one and quantizes them with GPTQ\n",
        "    Args:\n",
        "        checkpoint_path (os.PathLike): folder containing LLaMA weights\n",
        "        model (LlamaForCausalLM): model to dispatch layers into\n",
        "        batch (Tensor): sample model inputs\n",
        "        bits (int): number of bits to quantize to\n",
        "    \"\"\"\n",
        "    # Collect the first layer inputs, put them on .cuda() (the same as in get_batch_nll)\n",
        "    inps, attention_mask, position_ids = get_first_layer_inputs(model, batch)\n",
        "    inps = inps.cuda()\n",
        "    attention_mask = attention_mask.cuda()\n",
        "    position_ids = position_ids.cuda()\n",
        "\n",
        "    # Create a buffer for layer outputs\n",
        "    outs = torch.zeros_like(inps)\n",
        "\n",
        "    # Forward pass through the layers\n",
        "    layers = model.model.layers\n",
        "    assert len(layers) == 0\n",
        "    for i in trange(32):\n",
        "        # Load and dispatch the next layer\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].cuda()\n",
        "        linear_layers = find_layers(layer)\n",
        "\n",
        "        hessians = {name: None for name in linear_layers}\n",
        "        num_samples = {name: 0 for name in linear_layers}\n",
        "        handles = [\n",
        "            linear_layers[name].register_forward_hook(\n",
        "                get_accumulate_input_fn(name, hessians, num_samples)\n",
        "            ) for name in linear_layers\n",
        "        ]\n",
        "        forward_pass_layer(layer, inps, outs, attention_mask, position_ids)\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        for name, linear in linear_layers.items():\n",
        "            q, scale, zero = gptq(linear.weight.data, bits, 2 * hessians[name] / num_samples[name])\n",
        "            quantized_linear = QuantizedLinear(q, scale, zero, linear.bias)\n",
        "            replace_submodule(layer, name, quantized_linear)\n",
        "\n",
        "        forward_pass_layer(layer, inps, outs, attention_mask, position_ids)\n",
        "        inps, outs = outs, inps\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6lN_q-0zFej"
      },
      "source": [
        "**Evaluating the model with GPTQ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "c0be8771f8ff435dbc48b4895dc52d77",
            "1491cc683e784ff5894046fff22c1115",
            "c81bc1304e904256ab279d3987900930",
            "0ef9ab0cb610487b8918c5d5fb8358eb",
            "d7d00c41ab3740849ed5045784a72c3b",
            "846fb8143b754b8199063503386f6e17",
            "55efb3637e6d4e26bd0d9cfe3d2215cd",
            "6ff651a16f554b5090d675ba62289b06",
            "4cdc3eb9425d4c6aaa0d47873426d1e4",
            "c67462d724964032892e86acbc80a076",
            "170288bf62d24c2ba8f708097910c4a4"
          ]
        },
        "id": "GKRuiX7Vza96",
        "outputId": "fa4e5d1f-6f75-4f34-f9fe-53a87f4287b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-18d87c488859>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"pytorch_model-00033-of-00033.bin\")))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0be8771f8ff435dbc48b4895dc52d77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-18d87c488859>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"pytorch_model-{layer_idx + 1:05}-of-00033.bin\"))\n"
          ]
        }
      ],
      "source": [
        "model = initialize_layerless_llama(MODEL)\n",
        "llama_gptq(MODEL, model, train_batch, BITS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "875a6ce145c04cd989ef6f2a3eb18388",
            "3d24b7d136164309afd324f44ea6f418",
            "018761e2341144f5a0023efb618a45ea",
            "5f4b463c9f70404db57f21424523d7d6",
            "fa37a9e3cf59445c9b6cba625fe52cdf",
            "2790571930f34b019043327a676de337",
            "f3b94700216748948fc12a1d6ebc8392",
            "d726eac05a65428795e7a089ad094e51",
            "2e7098a87a27464f9c4029cd1e1eb898",
            "a7906a1532dc40c9826d15243579c204",
            "4ba78bb62abe4afbbe706a9f0cf5a5b3",
            "6c66fb2927d84bbf96f8ce19e6499d0d",
            "a325ecdfb8874ae2a08db38bf0cb69d5",
            "24ad5d71d60d417e9d663f5599686197",
            "9892b833c05c4de086e30ecd5b82db20",
            "a4f6106da6174e628a1a86d33584b72f",
            "09b97bb9fbf44fdc8dba9b0ca13ced66",
            "c64e9a0686994d4c92f1789478f02a2f",
            "637de288f34949c6b7bbe91814d3a6be",
            "77ed09c485854aeb9e79418b294a97e8",
            "74da3e1680f64936947a3d6932476843",
            "e9fd072ebffa4ca182bace176b717cb6",
            "cbf04484c37247b1a902fc904682d930",
            "a4d9c7a8285141c1837e848dddb3be64",
            "7764e1c6273c4db298615c65a290a8fe",
            "65f5bb29261b4269a183c68645daa466",
            "056a642be30a40c9a042adb96eefd8fa",
            "69b8a70c2ee04fa9950799c42a9e50de",
            "c06ce3ad2c9e47afa486af46d73345c4",
            "64e3a2f1908c4aae8154362c86e9125d",
            "746a1689c0c340ceabc548da20509c8f",
            "82e7e5a196dd451fb2986acf0a191e9a",
            "fa2bea8804ee4925b48a24dfe0dcd71c",
            "b49ee0885faa4e019d7816dc97e418e8",
            "92dad258a6884ae8b6a9c4d65d788882",
            "2f20c4969a0b4313b1d7100e64ccc0bc",
            "52e3a9eb20924463b79cb3275acd1ecb",
            "3fbb63a68fd24dc08fa67e636ffe7606",
            "3ecd0b6fa063456683c9be6922adc154",
            "cf466f5efc874ead8830eb27829e6e43",
            "298ae0969618434a8c9a15642641e357",
            "22854121137c41cf93650aad862f4a67",
            "d9fa2933c60d4a1d8a0df09c59d89ad5",
            "1dc9ff66c91d4892b32e0aaa161a0eea",
            "5fa6dae1f0ec4978a3410bf3a923a0b1",
            "9e007c4e3d6f4e26bd5b476b5e02a581",
            "223b84e13ee046f99c40f638969c165e",
            "7588d2ba19ca4cc28147e59eacaa09ef",
            "94e9e2cdfb024d90a79af252cb4c2c5e",
            "d20a63c97471422b90fbdd033466bb07",
            "3e7c542a7abc4c63a1424c810fbc3ae2",
            "e577fb5e78294d9b9371f118878748aa",
            "abe5ecf5955a416dbf26b49b80776471",
            "9213bbb86b42472f97a76f4a9253d21b",
            "5cfae6af9a594a419740ed94aceaf4e4"
          ]
        },
        "id": "53ARPLErza7N",
        "outputId": "bc638a38-4318-4649-fd85-88d665933ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "875a6ce145c04cd989ef6f2a3eb18388",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c66fb2927d84bbf96f8ce19e6499d0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbf04484c37247b1a902fc904682d930",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b49ee0885faa4e019d7816dc97e418e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fa6dae1f0ec4978a3410bf3a923a0b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPL: 5.958793260910872\n"
          ]
        }
      ],
      "source": [
        "gptq_ppl = llama_eval(model, test_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QwMLEqUza43",
        "outputId": "aaa2c2cd-920e-45de-c24b-e736378d8102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "assert gptq_ppl < 6.2\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "18BHd4Nnza2T"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KggEz3g37B1g"
      },
      "source": [
        "# Dense Integer Packing\n",
        "\n",
        "For the most part, we used `torch.uint8` to represent tensors quantized to *4 bits*. We encoded each *4-bit* value with an *8-bit* value, using twice as much memory as we really needed. We did this because `torch` lacks support for 4-bit tensors. However, we never performed any native operations in *8 bits*; we always converted it to `torch.float16` first. That means we can design a more efficient way to encode *4-bit* tensors and convert to and from it.\n",
        "\n",
        "Here we implement these conversions, converting *8-bit* tensors containing *4-bit* values (the way we used to store quantized weights) to and from smaller *8-bit* tensor, utilizing the whole range of values. Moreover, we do it in a way such that memory representation of contiguous arrays wouldn't change. That is, adjacent columns get squashed together.\n",
        "\n",
        "Implementation of dense *int4*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-jaEhv57MC9"
      },
      "outputs": [],
      "source": [
        "def dense_pack_4_8(x: Tensor) -> Tensor:\n",
        "    \"\"\"Constructs a densely packed int tensor\n",
        "    Args:\n",
        "        x (Tensor): uint8 with uint4 values range\n",
        "        real_bits (int): real values range of the tensor\n",
        "\n",
        "    Returns:\n",
        "        Tensor: twice as small uint8 tensor with uint8 values range\n",
        "    \"\"\"\n",
        "    x = x.clone().detach()\n",
        "    h, w = x.shape\n",
        "    dense_x = torch.zeros((h, (w + 1) // 2), dtype=torch.uint8)\n",
        "    for j in range(w):\n",
        "        j_d = j // 2\n",
        "        if j % 2 == 0:\n",
        "            dense_x[:, j_d] = x[:, j] * 16\n",
        "        else:\n",
        "            dense_x[:, j_d] += x[:, j]\n",
        "    return dense_x\n",
        "\n",
        "def dense_unpack_4_8(x: Tensor) -> Tensor:\n",
        "    \"\"\"Deconstructs a densely packed int tensor\n",
        "    Args:\n",
        "        x (Tensor): uint8 tensor with uint8 values range\n",
        "\n",
        "    Returns:\n",
        "        Tensor: twice as large int8 tensor with uint4 values range\n",
        "    \"\"\"\n",
        "    x = x.clone().detach()\n",
        "    h, w_d = x.shape\n",
        "    undense_x = torch.zeros((h, 2 * w_d), dtype=torch.uint8)\n",
        "\n",
        "    for j_d in range(w_d):\n",
        "        j_l , j_r = j_d * 2, j_d * 2 + 1\n",
        "\n",
        "        undense_x[:, j_l] = x[:, j_d] // 16\n",
        "        undense_x[:, j_r] = x[:, j_d] % 16\n",
        "\n",
        "    return undense_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sM4tAOU7Oez",
        "outputId": "84ef7e68-9f69-4b12-d375-d7fb161c9acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "quantized_x, scale, zero = measure_and_quantize(x, 4)\n",
        "dense_quantized_x = dense_pack_4_8(quantized_x)\n",
        "undense_quantized_x = dense_unpack_4_8(dense_quantized_x)\n",
        "\n",
        "assert dense_quantized_x.shape == (512, 512)\n",
        "assert torch.all(undense_quantized_x == quantized_x)\n",
        "assert (dense_quantized_x // 16 == dense_quantized_x % 16).float().mean() > 0.95, \"close values are supposed to be packed together\"\n",
        "print(\"All tests passed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "-1.-1.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
